{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "34e5ae83",
      "metadata": {
        "id": "34e5ae83"
      },
      "source": [
        "# Assignment 1: Wrangling and EDA\n",
        "### Foundations of Machine Learning"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "K97kK1hzWQnh"
      },
      "id": "K97kK1hzWQnh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bRcbo-4rGJql"
      },
      "outputs": [],
      "source": [
        "! git clone https://github.com/ds4e/scratchpad\n",
        "%run ./scratchpad/get_data.py"
      ],
      "id": "bRcbo-4rGJql"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "8eqiJKvxUR98"
      },
      "id": "8eqiJKvxUR98",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "metadata": {
        "id": "GMaGPhreUf0K"
      },
      "id": "GMaGPhreUf0K",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2rnGf0N4b4EY"
      },
      "id": "2rnGf0N4b4EY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "691fcb3e",
      "metadata": {
        "id": "691fcb3e"
      },
      "source": [
        "**Q1.** This question provides some practice cleaning variables which have common problems.\n",
        "1. Numeric variable: For `airbnb_NYC.csv`, clean the `Price` variable as well as you can, and explain the choices you make. How many missing values do you end up with? (Hint: What happens to the formatting when a price goes over 999 dollars, say from 675 to 1,112?)\n",
        "\n",
        "The Price variable was stored as an object because prices over 999 contain commas (e.g., “1,112”). I removed commas and converted the column to numeric, which allowed all values to be retained. After cleaning, there were 0 missing values.\n",
        "\n",
        "2. Categorical variable: For the Minnesota police use of for data, `mn_police_use_of_force.csv`, clean the `subject_injury` variable, handling the NA's; this gives a value `Yes` when a person was injured by police, and `No` when no injury occurred. What proportion of the values are missing? Cross-tabulate your cleaned `subject_injury` variable with the `force_type` variable. Are there any patterns regarding when the data are missing? For the remaining missing values, replace the `np.nan/None` values with the label `Missing`.\n",
        "\n",
        "The subject_injury variable was cleaned by keeping “Yes” and “No” values and labeling missing entries as “Missing.” About 76% of the values were missing. Cross-tabulation with force_type shows that missing injury data are more common for certain force types, such as Bodily Force and Chemical Irritant, suggesting that missingness is related to the type of force used rather than being random.\n",
        "\n",
        "3. Dummy variable: For `metabric.csv`, convert the `Overall Survival Status` variable into a dummy/binary variable, taking the value 0 if the patient is deceased and 1 if they are living.\n",
        "\n",
        "The Overall Survival Status variable was converted into a binary variable by assigning a value of 0 to deceased patients and 1 to living patients. This required matching the original string labels in the dataset (1:DECEASED and 0:LIVING) to their corresponding numeric values. Creating this dummy variable makes the survival outcome easier to use in analysis and modeling.\n",
        "\n",
        "4. Missing values: For `airbnb_NYC.csv`, determine how many missing values of `Review Scores Rating` there are. Create a new variable, in which you impute the median score for non-missing observations to the missing ones. Why might this bias or otherwise negatively impact your results?\n",
        "\n",
        "There are 8,323 missing values in the Review Scores Rating variable. I created a new variable by imputing the median rating of 94 for the missing values. This approach can bias results by reducing variability and hiding meaningful differences between listings, especially if missing ratings are not random."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "airbnb = pd.read_csv(\"airbnb_NYC.csv\")"
      ],
      "metadata": {
        "id": "EXjTKBNQWha8"
      },
      "id": "EXjTKBNQWha8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "airbnb = pd.read_csv(\"airbnb_NYC.csv\", encoding=\"latin1\")"
      ],
      "metadata": {
        "id": "f0dr_qz4XZkz"
      },
      "id": "f0dr_qz4XZkz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "airbnb.columns"
      ],
      "metadata": {
        "id": "hCfFi1-TXs70"
      },
      "id": "hCfFi1-TXs70",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wfTA2vHVX31w"
      },
      "id": "wfTA2vHVX31w",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "airbnb[\"Price\"].head(10)\n"
      ],
      "metadata": {
        "id": "sd7LK7tWXxHO"
      },
      "id": "sd7LK7tWXxHO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PoeQeJNZX40_"
      },
      "id": "PoeQeJNZX40_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "airbnb.head()"
      ],
      "metadata": {
        "id": "H9lyMUY_XnOq"
      },
      "id": "H9lyMUY_XnOq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "airbnb[\"Price\"].head(10)\n",
        "airbnb[\"Price\"].dtype"
      ],
      "metadata": {
        "id": "2lR27rMgYVxV"
      },
      "id": "2lR27rMgYVxV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "airbnb[\"Price_clean\"] = (\n",
        "    airbnb[\"Price\"]\n",
        "    .astype(str)\n",
        "    .str.replace(\",\", \"\", regex=False)\n",
        "    .astype(float)\n",
        ")\n"
      ],
      "metadata": {
        "id": "LMOBc_cuYrr4"
      },
      "id": "LMOBc_cuYrr4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "airbnb[\"Price_clean\"].dtype\n",
        "airbnb[\"Price_clean\"].isna().sum()"
      ],
      "metadata": {
        "id": "V_RmpXBPYvQJ"
      },
      "id": "V_RmpXBPYvQJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mn = pd.read_csv(\"mn_police_use_of_force.csv\", encoding=\"latin1\")"
      ],
      "metadata": {
        "id": "iyH7MBFKZCY7"
      },
      "id": "iyH7MBFKZCY7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6vQtG8YmZY0v"
      },
      "id": "6vQtG8YmZY0v",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "metadata": {
        "id": "a3qWoNNYZZXt"
      },
      "execution_count": null,
      "outputs": [],
      "id": "a3qWoNNYZZXt"
    },
    {
      "cell_type": "code",
      "source": [
        "mn = pd.read_csv(\"mn_police_use_of_force.csv\", encoding=\"latin1\")"
      ],
      "metadata": {
        "id": "s9JuSbUGZi3B"
      },
      "id": "s9JuSbUGZi3B",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mn.head()"
      ],
      "metadata": {
        "id": "ItwGni5xZm4F"
      },
      "id": "ItwGni5xZm4F",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mn[\"subject_injury\"].value_counts(dropna=False)"
      ],
      "metadata": {
        "id": "K3xIWmR6ZqLI"
      },
      "id": "K3xIWmR6ZqLI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mn[\"subject_injury_clean\"] = mn[\"subject_injury\"].replace({\n",
        "    \"Yes\": \"Yes\",\n",
        "    \"No\": \"No\",\n",
        "    np.nan: \"Missing\"\n",
        "})"
      ],
      "metadata": {
        "id": "SfZxepEzZ7de"
      },
      "id": "SfZxepEzZ7de",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mn[\"subject_injury_clean\"].value_counts()"
      ],
      "metadata": {
        "id": "npRnXassZ9_s"
      },
      "id": "npRnXassZ9_s",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "missing_prop = (mn[\"subject_injury_clean\"] == \"Missing\").mean()\n",
        "missing_prop"
      ],
      "metadata": {
        "id": "kZaZuQXgaCB7"
      },
      "id": "kZaZuQXgaCB7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.crosstab(mn[\"force_type\"], mn[\"subject_injury_clean\"])"
      ],
      "metadata": {
        "id": "KKzwr5cVaHe2"
      },
      "id": "KKzwr5cVaHe2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "metadata": {
        "id": "Jmopk5pwcNHh"
      },
      "id": "Jmopk5pwcNHh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "meta = pd.read_csv(\"metabric.csv\", encoding=\"latin1\")"
      ],
      "metadata": {
        "id": "78B1wbUTcTw2"
      },
      "id": "78B1wbUTcTw2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "meta[\"Overall Survival Status\"].value_counts()"
      ],
      "metadata": {
        "id": "Zy6n6-f-cWQs"
      },
      "id": "Zy6n6-f-cWQs",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "meta[\"survival_binary\"] = meta[\"Overall Survival Status\"].map({\n",
        "    \"Deceased\": 0,\n",
        "    \"Living\": 1\n",
        "})"
      ],
      "metadata": {
        "id": "hNYXWL5ucbZw"
      },
      "id": "hNYXWL5ucbZw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "meta[[\"Overall Survival Status\", \"survival_binary\"]].head()"
      ],
      "metadata": {
        "id": "M467GOBdcemG"
      },
      "id": "M467GOBdcemG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "meta[\"survival_binary\"].value_counts()"
      ],
      "metadata": {
        "id": "CXABa68ucgLk"
      },
      "id": "CXABa68ucgLk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "meta[\"survival_binary\"] = meta[\"Overall Survival Status\"].map({\n",
        "    \"1:DECEASED\": 0,\n",
        "    \"0:LIVING\": 1\n",
        "})"
      ],
      "metadata": {
        "id": "dhAur2Elc0DZ"
      },
      "id": "dhAur2Elc0DZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "meta[[\"Overall Survival Status\", \"survival_binary\"]].head()"
      ],
      "metadata": {
        "id": "mwp9UoTbc2l5"
      },
      "id": "mwp9UoTbc2l5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "meta[\"survival_binary\"].value_counts()"
      ],
      "metadata": {
        "id": "3eBHWGXmc56_"
      },
      "id": "3eBHWGXmc56_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "airbnb[\"Review Scores Rating\"].isna().sum()"
      ],
      "metadata": {
        "id": "arv79l3idKgJ"
      },
      "id": "arv79l3idKgJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "median_rating = airbnb[\"Review Scores Rating\"].median()\n",
        "median_rating"
      ],
      "metadata": {
        "id": "sD2Qwmv4dPWF"
      },
      "id": "sD2Qwmv4dPWF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "airbnb[\"Review_Scores_Imputed\"] = airbnb[\"Review Scores Rating\"].fillna(median_rating)"
      ],
      "metadata": {
        "id": "2QIkLtxhdVO2"
      },
      "id": "2QIkLtxhdVO2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "airbnb[\"Review_Scores_Imputed\"].isna().sum()"
      ],
      "metadata": {
        "id": "4ybxqCE7dYep"
      },
      "id": "4ybxqCE7dYep",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mlHOC_JCiCem"
      },
      "id": "mlHOC_JCiCem",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "772a66bd",
      "metadata": {
        "id": "772a66bd"
      },
      "source": [
        "**Q2.** Go to https://sharkattackfile.net/ and download their dataset on shark attacks.\n",
        "\n",
        "1. Open the shark attack file using Pandas. It is probably not a csv file, so `read_csv` won't work. What does work?\n",
        "\n",
        "The shark attack dataset is an Excel file rather than a CSV, so read_csv does not work. Instead, I used pd.read_excel() to load the data successfully into pandas and verified it by viewing the first few rows and the dataset shape.\n",
        "\n",
        "2. Drop any columns that do not contain data.\n",
        "\n",
        "After inspecting missing values by column, I found that several columns contained no meaningful data. I dropped columns that were entirely missing using dropna(axis=1, how=\"all\"), which reduced the dataset to only columns with at least some observed values.\n",
        "\n",
        "3. What is an observation? Carefully justify your answer, and explain how it affects your choices in cleaning and analyzing the data.\n",
        "\n",
        "Each observation represents a single shark attack incident, with one row corresponding to one event. This affects the analysis by guiding cleaning decisions to preserve one row per attack while standardizing variables like year, age, attack type, and fatality status for consistent comparison across incidents.\n",
        "\n",
        "4. Clean the year variable. Describe the range of values you see. Filter the rows to focus on attacks since 1940. Are attacks increasing, decreasing, or remaining constant over time?\n",
        "\n",
        "The Year variable was cleaned by converting it to numeric and removing invalid values. The cleaned year values range from 1940 to recent years. After filtering the data to include only attacks since 1940, the number of recorded shark attacks generally increases over time, with noticeable year-to-year variation. The apparent drop in the most recent years is likely due to incomplete reporting rather than an actual decrease in attacks.\n",
        "\n",
        "5. Clean the Age variable and make a histogram of the ages of the victims.\n",
        "6. Clean the `Type` variable so it only takes three values: Provoked and Unprovoked and Unknown. What proportion of attacks are unprovoked?\n",
        "\n",
        "74%\n",
        "\n",
        "7. Clean the `Fatal Y/N` variable so it only takes three values: Y, N, and Unknown.\n",
        "8. Is the attack more or less likely to be fatal when the attack is provoked or unprovoked? Thoughts?\n",
        "\n",
        "Unprovoked shark attacks are more likely to be fatal than provoked attacks. The cross-tabulation shows that about 24% of unprovoked attacks result in fatalities, compared to only about 3% of provoked attacks. This suggests that unprovoked encounters tend to be more severe, while provoked incidents are less likely to lead to fatal outcomes."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "metadata": {
        "id": "lxMGVHShfMLM"
      },
      "id": "lxMGVHShfMLM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sharks = pd.read_excel(\"GSAF5.xls\")"
      ],
      "metadata": {
        "id": "AYfW_d6Tfnqz"
      },
      "id": "AYfW_d6Tfnqz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sharks.head()\n",
        "sharks.shape"
      ],
      "metadata": {
        "id": "0uuzHkKIftLs"
      },
      "id": "0uuzHkKIftLs",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sharks.isna().sum().sort_values(ascending=False).head(20)"
      ],
      "metadata": {
        "id": "TRvIFJhKfwPI"
      },
      "id": "TRvIFJhKfwPI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sharks.isna().sum().sort_values(ascending=False).head(20)"
      ],
      "metadata": {
        "id": "__B9cd5mfyNL"
      },
      "id": "__B9cd5mfyNL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sharks = sharks.dropna(axis=1, how=\"all\")"
      ],
      "metadata": {
        "id": "T3qHfwS9f8z6"
      },
      "id": "T3qHfwS9f8z6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sharks.shape"
      ],
      "metadata": {
        "id": "FFWj1ZBxgAgn"
      },
      "id": "FFWj1ZBxgAgn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sharks[\"Year\"].head(10)\n",
        "sharks[\"Year\"].dtype\n"
      ],
      "metadata": {
        "id": "WmlfURc_hBZl"
      },
      "id": "WmlfURc_hBZl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sharks[\"Year_clean\"] = pd.to_numeric(sharks[\"Year\"], errors=\"coerce\")\n"
      ],
      "metadata": {
        "id": "Q10HflqohFqD"
      },
      "id": "Q10HflqohFqD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sharks[\"Year_clean\"].describe()"
      ],
      "metadata": {
        "id": "hIC843SThIUy"
      },
      "id": "hIC843SThIUy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sharks_since_1940 = sharks[sharks[\"Year_clean\"] >= 1940]"
      ],
      "metadata": {
        "id": "cqJUYVXihK8o"
      },
      "id": "cqJUYVXihK8o",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sharks_since_1940.shape"
      ],
      "metadata": {
        "id": "xHFnrYzQhN87"
      },
      "id": "xHFnrYzQhN87",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sharks_since_1940[\"Year_clean\"].value_counts().sort_index().plot()"
      ],
      "metadata": {
        "id": "4NG9YUvThROS"
      },
      "id": "4NG9YUvThROS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sharks[\"Age\"].head(10)\n",
        "sharks[\"Age\"].dtype"
      ],
      "metadata": {
        "id": "xbMUYKwjiJwQ"
      },
      "id": "xbMUYKwjiJwQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sharks[\"Age_clean\"] = pd.to_numeric(sharks[\"Age\"], errors=\"coerce\")"
      ],
      "metadata": {
        "id": "Z-7TutCdiMUs"
      },
      "id": "Z-7TutCdiMUs",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sharks[\"Age_clean\"].describe()"
      ],
      "metadata": {
        "id": "aOLFIn4niPjm"
      },
      "id": "aOLFIn4niPjm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sharks[\"Age_clean\"].dropna().plot(kind=\"hist\", bins=20)"
      ],
      "metadata": {
        "id": "Co44_OKiiSEM"
      },
      "id": "Co44_OKiiSEM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sharks[\"Type\"].value_counts(dropna=False)\n"
      ],
      "metadata": {
        "id": "qAD9pN5Figzn"
      },
      "id": "qAD9pN5Figzn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sharks[\"Type_clean\"] = sharks[\"Type\"].replace({\n",
        "    \"Provoked\": \"Provoked\",\n",
        "    \"Unprovoked\": \"Unprovoked\"\n",
        "})\n"
      ],
      "metadata": {
        "id": "BX-1aDH0ilm2"
      },
      "id": "BX-1aDH0ilm2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sharks[\"Type_clean\"] = sharks[\"Type_clean\"].fillna(\"Unknown\")\n"
      ],
      "metadata": {
        "id": "p6bsU-8iimgM"
      },
      "id": "p6bsU-8iimgM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sharks[\"Type_clean\"].value_counts()\n"
      ],
      "metadata": {
        "id": "e89_nZ0oiqAs"
      },
      "id": "e89_nZ0oiqAs",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sharks[\"Type_clean\"].value_counts()\n"
      ],
      "metadata": {
        "id": "XWmvgSrMivRh"
      },
      "id": "XWmvgSrMivRh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sharks[\"Type_clean\"] = sharks[\"Type\"].str.strip().str.capitalize()\n"
      ],
      "metadata": {
        "id": "WB960r9ri7NZ"
      },
      "id": "WB960r9ri7NZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sharks[\"Type_clean\"] = sharks[\"Type_clean\"].where(\n",
        "    sharks[\"Type_clean\"].isin([\"Provoked\", \"Unprovoked\"]),\n",
        "    \"Unknown\"\n",
        ")"
      ],
      "metadata": {
        "id": "8ICwQpXai-ZN"
      },
      "id": "8ICwQpXai-ZN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sharks[\"Type_clean\"].value_counts()\n"
      ],
      "metadata": {
        "id": "O-lAKOqIjAj1"
      },
      "id": "O-lAKOqIjAj1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sharks[\"Fatal Y/N\"].value_counts(dropna=False)\n"
      ],
      "metadata": {
        "id": "AtrXGRzBjUwD"
      },
      "id": "AtrXGRzBjUwD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sharks[\"Fatal_clean\"] = sharks[\"Fatal Y/N\"].str.strip().str.upper()\n"
      ],
      "metadata": {
        "id": "P_E6GE8LjXY4"
      },
      "id": "P_E6GE8LjXY4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sharks[\"Fatal_clean\"] = sharks[\"Fatal_clean\"].where(\n",
        "    sharks[\"Fatal_clean\"].isin([\"Y\", \"N\"]),\n",
        "    \"Unknown\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "DZwvR4agjYZ8"
      },
      "id": "DZwvR4agjYZ8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sharks[\"Fatal_clean\"].value_counts()\n"
      ],
      "metadata": {
        "id": "QTijBvE0jb22"
      },
      "id": "QTijBvE0jb22",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.crosstab(\n",
        "    sharks[\"Type_clean\"],\n",
        "    sharks[\"Fatal_clean\"],\n",
        "    normalize=\"index\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "07rqHJ6yjjiE"
      },
      "id": "07rqHJ6yjjiE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "0e54a370",
      "metadata": {
        "id": "0e54a370"
      },
      "source": [
        "**Q3.** Open the \"tidy_data.pdf\" document available in `https://github.com/ds4e/wrangling`, which is a paper called *Tidy Data* by Hadley Wickham.\n",
        "\n",
        "  1. Read the abstract. What is this paper about?\n",
        "\n",
        "  The paper argues that many difficulties in data analysis come from poor data structure rather than from modeling or statistical methods. Wickham introduces the concept of tidy data and proposes a simple, consistent way to organize datasets so they are easier to manipulate, visualize, and analyze.\n",
        "\n",
        "  2. Read the introduction. What is the \"tidy data standard\" intended to accomplish?\n",
        "\n",
        "  The tidy data standard is meant to reduce friction in data analysis by providing a common structure for datasets. By organizing data so that variables, observations, and values are clearly defined and consistently placed, the standard makes data cleaning more systematic and allows analysts to spend less time reshaping data and more time analyzing it.\n",
        "\n",
        "  3. Read the intro to section 2. What does this sentence mean: \"Like families, tidy datasets are all alike but every messy dataset is messy in its own way.\" What does this sentence mean: \"For a given dataset, it’s usually easy to figure out what are observations and what are variables, but it is surprisingly difficult to precisely define variables and observations in general.\"\n",
        "\n",
        "  The statement “Like families, tidy datasets are all alike but every messy dataset is messy in its own way” means that tidy datasets follow a shared structure, while messy datasets can be disorganized in many different and unpredictable forms. The follow-up sentence emphasizes that while it is often easy to roughly identify what the variables and observations are in a dataset, it is much harder to define them precisely and consistently. This lack of precision is what leads to messy data structures that complicate analysis.\n",
        "\n",
        "  4. Read Section 2.2. How does Wickham define values, variables, and observations?\n",
        "\n",
        "  Values as the actual measurements or entries in the dataset. Variables as the attributes being measured (what is recorded). Observations as a single unit across all variables (what the measurements describe).\n",
        "\n",
        "  5. How is \"Tidy Data\" defined in section 2.3?\n",
        "  \n",
        "  Wickham defines tidy data using three rules: each variable must have its own column, each observation must have its own row, and each type of observational unit must form its own table. When these rules are followed, data become easier to transform, visualize, and model.\n",
        "\n",
        "  6. Read the intro to Section 3 and Section 3.1. What are the 5 most common problems with messy datasets? Why are the data in Table 4 messy? What is \"melting\" a dataset?\n",
        "\n",
        "  Wickham identifies five common problems with messy datasets: column headers that contain values instead of variable names, multiple variables stored in a single column, variables stored in both rows and columns, multiple types of observational units combined in one table, and a single observational unit split across multiple tables. Table 4 is messy because it violates these tidy data principles, particularly by embedding values in column headers and mixing multiple variables within the same structure, which makes it difficult to clearly identify variables and observations and complicates analysis.\n",
        "\n",
        "  7. Why, specifically, is table 11 messy but table 12 tidy and \"molten\"?\n",
        "\n",
        "  Table 11 is messy because it spreads observations across multiple columns and embeds variable names as values, making the structure difficult to analyze. Table 12 is tidy and “molten” because the data have been reshaped into a long format where each variable has its own column and each observation has its own row. This molten structure follows tidy data principles and supports more flexible analysis."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YKqsAY3tflTE"
      },
      "id": "YKqsAY3tflTE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "a58495aa",
      "metadata": {
        "id": "a58495aa"
      },
      "source": [
        "**Q4.** This question looks at financial transfers from international actors to American universities. In particular, from which countries and giftors are the gifts coming from, and to which institutions are they going?\n",
        "\n",
        "For this question, `.groupby([vars]).count()` and `.groupby([vars]).sum()` will be especially useful to tally the number of occurrences and sum the values of those occurrences.\n",
        "\n",
        "1. Load the `ForeignGifts_edu.csv` dataset.\n",
        "\n",
        "The ForeignGifts_edu.csv dataset was loaded into a pandas DataFrame called gifts. The dataset contains information on foreign financial transfers to U.S. higher education institutions, including the institution name, gift amount, gift type, country of the giftor, and the giftor name. Each row represents a reported foreign gift or contract.\n",
        "\n",
        "2. For `Foreign Gift Amount`, create a histogram and describe the variable. Describe your findings.\n",
        "\n",
        "The variable Foreign Gift Amount is highly right-skewed. Most foreign gifts are relatively small in dollar value, while a small number of gifts are extremely large, reaching into the hundreds of millions of dollars. This indicates that although many foreign gifts occur, the total amount of funding is driven by a small number of very large transfers rather than typical-sized gifts.\n",
        "\n",
        "3. For `Gift Type`, create a histogram or value counts table. What proportion of the gifts are contracts, real estate, and monetary gifts?\n",
        "\n",
        "When examining Gift Type, the majority of foreign financial transfers fall into monetary gifts and contracts, with real estate gifts making up a much smaller proportion. Contracts account for a substantial share of the total value, reflecting formal agreements between foreign entities and U.S. institutions, while purely monetary gifts are more frequent but vary widely in size.\n",
        "\n",
        "4. What are the top 15 countries in terms of the number of gifts? What are the top 15 countries in terms of the amount given?\n",
        "\n",
        "In terms of the number of gifts, England, China, and Canada are the most frequent sources of foreign gifts to U.S. universities. However, when ranked by the total dollar amount, the ordering changes significantly. Qatar contributes the largest total amount despite having fewer gifts, followed by England, China, and Saudi Arabia. This shows that some countries provide fewer but much larger gifts, while others contribute more frequently but in smaller amounts.\n",
        "\n",
        "5. What are the top 15 institutions in terms of the total amount of money they receive? Make a histogram of the total amount received by all institutions.\n",
        "\n",
        "Foreign gift funding is heavily concentrated among a small number of elite institutions. Carnegie Mellon University and Cornell University receive the largest total amounts of foreign funding, followed by Harvard University and MIT. The histogram of total foreign gifts by institution is extremely right-skewed, showing that most institutions receive relatively small amounts, while a small number receive exceptionally large sums.\n",
        "\n",
        "6. Which giftors provide the most money, in total?\n",
        "\n",
        "The largest foreign giftors are dominated by a small group of organizations. The Qatar Foundation is the single largest contributor by total amount, followed by other Qatar-affiliated entities such as the Qatar National Research Fund and Qatar Foundation for Education. Several Saudi-affiliated organizations and government entities also appear among the top contributors, along with a notable amount of funding coming from anonymous donors. Overall, foreign funding is concentrated among a limited set of large, often state-linked or foundation-based giftors."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "b1P2F6a4JgXN"
      },
      "id": "b1P2F6a4JgXN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "metadata": {
        "id": "TKScvTz9QW6s"
      },
      "id": "TKScvTz9QW6s",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gifts = pd.read_csv(\"ForeignGifts_edu.csv\")\n",
        "gifts.head()"
      ],
      "metadata": {
        "id": "YEJh3xtIQnVX"
      },
      "id": "YEJh3xtIQnVX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gifts[\"Foreign Gift Amount\"].describe()"
      ],
      "metadata": {
        "id": "m6ft9CsoQslY"
      },
      "id": "m6ft9CsoQslY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gifts[\"Foreign Gift Amount\"].plot(kind=\"hist\", bins=50)\n",
        "plt.xlabel(\"Foreign Gift Amount\")\n",
        "plt.title(\"Distribution of Foreign Gift Amounts\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "awCeFHTRQwPG"
      },
      "id": "awCeFHTRQwPG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gift_counts = gifts[\"Gift Type\"].value_counts()\n",
        "gift_counts"
      ],
      "metadata": {
        "id": "NtH3auckQ7FU"
      },
      "id": "NtH3auckQ7FU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gift_counts / gift_counts.sum()"
      ],
      "metadata": {
        "id": "yjOtg4vQQ910"
      },
      "id": "yjOtg4vQQ910",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "top_countries_count = (\n",
        "    gifts.groupby(\"Country\")\n",
        "    .size()\n",
        "    .sort_values(ascending=False)\n",
        "    .head(15)\n",
        ")\n",
        "\n",
        "top_countries_count"
      ],
      "metadata": {
        "id": "V_BSin6fRBRR"
      },
      "id": "V_BSin6fRBRR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gifts.columns"
      ],
      "metadata": {
        "id": "EBFbjwbuRN4T"
      },
      "id": "EBFbjwbuRN4T",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "top_countries_count = (\n",
        "    gifts.groupby(\"Country of Gift Origin\")\n",
        "    .size()\n",
        "    .sort_values(ascending=False)\n",
        "    .head(15)\n",
        ")\n",
        "\n",
        "top_countries_count"
      ],
      "metadata": {
        "id": "7d8g9-35RaoW"
      },
      "id": "7d8g9-35RaoW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "top_countries_count = (\n",
        "    gifts.groupby(\"Country of Giftor\")\n",
        "    .size()\n",
        "    .sort_values(ascending=False)\n",
        "    .head(15)\n",
        ")\n",
        "\n",
        "top_countries_count"
      ],
      "metadata": {
        "id": "5FRPAHtcSfYD"
      },
      "id": "5FRPAHtcSfYD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "top_countries_amount = (\n",
        "    gifts.groupby(\"Country of Giftor\")[\"Foreign Gift Amount\"]\n",
        "    .sum()\n",
        "    .sort_values(ascending=False)\n",
        "    .head(15)\n",
        ")\n",
        "\n",
        "top_countries_amount"
      ],
      "metadata": {
        "id": "WWeqAXEOSiGO"
      },
      "id": "WWeqAXEOSiGO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "top_institutions = (\n",
        "    gifts.groupby(\"Institution Name\")[\"Foreign Gift Amount\"]\n",
        "    .sum()\n",
        "    .sort_values(ascending=False)\n",
        "    .head(15)\n",
        ")\n",
        "\n",
        "top_institutions"
      ],
      "metadata": {
        "id": "9h2JblbLTuAb"
      },
      "id": "9h2JblbLTuAb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "institution_totals = (\n",
        "    gifts.groupby(\"Institution Name\")[\"Foreign Gift Amount\"]\n",
        "    .sum()\n",
        ")"
      ],
      "metadata": {
        "id": "C6yzPiU3TxJ4"
      },
      "id": "C6yzPiU3TxJ4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "institution_totals.plot(kind=\"hist\", bins=50)\n",
        "plt.xlabel(\"Total Foreign Gift Amount Received\")\n",
        "plt.title(\"Distribution of Total Foreign Gifts by Institution\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "aPD1MMDpUI8a"
      },
      "id": "aPD1MMDpUI8a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "top_giftors = (\n",
        "    gifts.groupby(\"Giftor Name\")[\"Foreign Gift Amount\"]\n",
        "    .sum()\n",
        "    .sort_values(ascending=False)\n",
        "    .head(15)\n",
        ")\n",
        "\n",
        "top_giftors"
      ],
      "metadata": {
        "id": "JHfpzb2nU7Fw"
      },
      "id": "JHfpzb2nU7Fw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "college.shape\n",
        "college.head()"
      ],
      "metadata": {
        "id": "Eh1EM78Wglk-"
      },
      "id": "Eh1EM78Wglk-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Zjq5zqlIgvz7"
      },
      "id": "Zjq5zqlIgvz7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "465a8e97",
      "metadata": {
        "id": "465a8e97"
      },
      "source": [
        "**Q5.** This question provides some practice doing exploratory data analysis and visualization.\n",
        "\n",
        "We'll use the `college_completion.csv` dataset from the US Department of Education. The \"relevant\" variables for this question are:\n",
        "  - `level` - Level of institution (4-year, 2-year)\n",
        "  - `aid_value` - The average amount of student aid going to undergraduate recipients\n",
        "  - `control` - Public, Private not-for-profit, Private for-profit\n",
        "  - `grad_100_value` - percentage of first-time, full-time, degree-seeking undergraduates who complete a degree or certificate program within 100 percent of expected time (bachelor's-seeking group at 4-year institutions)\n",
        "\n",
        "1. Load the `college_completion.csv` data with Pandas.\n",
        "2. How many observations and variables are in the data? Use `.head()` to examine the first few rows of data.\n",
        "\n",
        "The dataset contains 3,798 observations and 63 variables. Each observation corresponds to a U.S. postsecondary institution, and the variables capture institutional characteristics including location, level, control, student aid, and graduation outcomes. Inspecting the first few rows confirms that the data include a mix of public and private institutions across different states and institutional types.\n",
        "\n",
        "3. Cross tabulate `control` and `level`. Describe the patterns you see in words.\n",
        "\n",
        "The cross-tabulation shows clear differences in institutional structure across control types. Private not-for-profit institutions are overwhelmingly 4-year schools, with about 94.6% offering 4-year programs and only 5.4% being 2-year institutions. Public institutions are more evenly split, though they lean toward 2-year colleges, with roughly 59.4% being 2-year and 40.6% being 4-year, reflecting the prevalence of community colleges in the public sector. Private for-profit institutions are the most balanced across levels, with about 53.1% being 4-year and 46.9% being 2-year institutions. Overall, institutional control is strongly associated with institutional level, with not-for-profit schools concentrated in the 4-year sector and public schools playing a major role in 2-year education.\n",
        "\n",
        "4. For `grad_100_value`, create a kernel density plot and describe table. Now condition on `control`, and produce a kernel density plot and describe tables for each type of institutional control. Which type of institution appear to have the most favorable graduation rates?\n",
        "\n",
        "They have the highest mean and median graduation rates, and their distribution is shifted to the right compared to public and private for-profit schools. Public institutions have the lowest graduation rates overall, while private for-profit institutions fall in between but show more variability.\n",
        "\n",
        "5. Make a scatterplot of `grad_100_value` by `aid_value`, and compute the covariance and correlation between the two variables. Describe what you see. Now make the same plot and statistics, but conditioning on `control`. Describe what you see. For which kinds of institutions does aid seem to vary positively with graduation rates?\n",
        "\n",
        "Looking at the scatterplot, there’s an overall upward trend between average student aid and graduation rates, but it’s pretty noisy. When breaking it out by institutional control, the pattern becomes clearer. Private not-for-profit schools show the strongest positive relationship between aid and graduation rates, where higher aid generally lines up with higher completion rates. Public institutions also show a positive relationship, though it’s weaker and more spread out. Private for-profit schools show only a very weak relationship, with graduation rates staying low even as aid increases. Overall, aid seems to vary most positively with graduation rates at private not-for-profit institutions, followed by public schools, and least for private for-profit schools."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "metadata": {
        "id": "z6riXnBefrTc"
      },
      "id": "z6riXnBefrTc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "college = pd.read_csv(\"college_completion.csv\")\n"
      ],
      "metadata": {
        "id": "plOxoFBrgJkz"
      },
      "id": "plOxoFBrgJkz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "college.shape"
      ],
      "metadata": {
        "id": "_Wiflh3Qg1SK"
      },
      "id": "_Wiflh3Qg1SK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.crosstab(college[\"control\"], college[\"level\"])"
      ],
      "metadata": {
        "id": "lpOfI8AhhWOv"
      },
      "id": "lpOfI8AhhWOv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.crosstab(college[\"control\"], college[\"level\"], normalize=\"index\")"
      ],
      "metadata": {
        "id": "UXhBnHgAhcGY"
      },
      "id": "UXhBnHgAhcGY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "college[\"grad_100_value\"].describe()"
      ],
      "metadata": {
        "id": "QgaVMLywh4Io"
      },
      "id": "QgaVMLywh4Io",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "college[\"grad_100_value\"].plot(kind=\"kde\")\n",
        "plt.xlabel(\"Graduation rate (100% time)\")\n",
        "plt.title(\"Kernel Density of Graduation Rates\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "dA183sGsh66M"
      },
      "id": "dA183sGsh66M",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "college.groupby(\"control\")[\"grad_100_value\"].describe()"
      ],
      "metadata": {
        "id": "PiNnryfRh78I"
      },
      "id": "PiNnryfRh78I",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "college.groupby(\"control\")[\"grad_100_value\"].plot(kind=\"kde\", legend=True)\n",
        "plt.xlabel(\"Graduation rate (100% time)\")\n",
        "plt.title(\"Graduation Rates by Institutional Control\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "-mFVFT9viA7i"
      },
      "id": "-mFVFT9viA7i",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.scatter(college[\"aid_value\"], college[\"grad_100_value\"], alpha=0.4)\n",
        "plt.xlabel(\"Average Student Aid (aid_value)\")\n",
        "plt.ylabel(\"Graduation Rate (100% time)\")\n",
        "plt.title(\"Graduation Rate vs Student Aid\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "tTqe3ze6i4Dn"
      },
      "id": "tTqe3ze6i4Dn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cov = college[[\"aid_value\", \"grad_100_value\"]].cov()\n",
        "corr = college[[\"aid_value\", \"grad_100_value\"]].corr()\n",
        "\n",
        "cov, corr"
      ],
      "metadata": {
        "id": "gt0D_dFzi6a4"
      },
      "id": "gt0D_dFzi6a4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for c in college[\"control\"].unique():\n",
        "    subset = college[college[\"control\"] == c]\n",
        "    plt.scatter(subset[\"aid_value\"], subset[\"grad_100_value\"], alpha=0.5, label=c)\n",
        "\n",
        "plt.xlabel(\"Average Student Aid (aid_value)\")\n",
        "plt.ylabel(\"Graduation Rate (100% time)\")\n",
        "plt.title(\"Graduation Rate vs Aid by Institutional Control\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "r-ycEv0wi9eT"
      },
      "id": "r-ycEv0wi9eT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "college.groupby(\"control\")[[\"aid_value\", \"grad_100_value\"]].corr()"
      ],
      "metadata": {
        "id": "EhrG_XFrjAMp"
      },
      "id": "EhrG_XFrjAMp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "college.groupby(\"control\").apply(\n",
        "    lambda df: df[\"aid_value\"].corr(df[\"grad_100_value\"])\n",
        ")"
      ],
      "metadata": {
        "id": "KIvPNtN8jCqZ"
      },
      "id": "KIvPNtN8jCqZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n"
      ],
      "metadata": {
        "id": "xEPQ2zcvknCB"
      },
      "id": "xEPQ2zcvknCB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "8b1a468b",
      "metadata": {
        "id": "8b1a468b"
      },
      "source": [
        "**Q6.** In class, we talked about how to compute the sample mean of a variable $X$,\n",
        "$$\n",
        "m(X) = \\dfrac{1}{N} \\sum_{i=1}^N x_i\n",
        "$$\n",
        "and sample covariance of two variables $X$ and $Y$,\n",
        "$$\n",
        "\\text{cov}(X,Y) = \\dfrac{1}{N} \\sum_{i=1}^N (x_i - m(X))(y_i - m(Y))).\n",
        "$$\n",
        "Recall, the sample variance of $X$ is\n",
        "$$\n",
        "s^2 = \\dfrac{1}{N} \\sum_{i=1}^N (x_i - m(X))^2.\n",
        "$$\n",
        "It can be very helpful to understand some basic properties of these statistics. If you want to write your calculations on a piece of paper, take a photo, and upload that to your GitHub repo, that's probably easiest.\n",
        "\n",
        "We're going to look at **linear transformations** of $X$, $Y = a + bX$. So we take each value of $X$, $x_i$, and transform it as $y_i = a + b x_i$.\n",
        "\n",
        "1. Show that $m(a + bX) = a+b \\times m(X)$.\n",
        "\n",
        "For a linear transformation\n",
        "𝑎\n",
        "+\n",
        "𝑏\n",
        "𝑋\n",
        "a+bX, adding\n",
        "𝑎\n",
        "a shifts every value of\n",
        "𝑋\n",
        "X by the same amount and multiplying by\n",
        "𝑏\n",
        "b scales every value. Since the mean is just the average of all values, the mean also shifts by\n",
        "𝑎\n",
        "a and scales by\n",
        "𝑏\n",
        "b. Therefore, the mean of\n",
        "𝑎\n",
        "+\n",
        "𝑏\n",
        "𝑋\n",
        "a+bX is\n",
        "𝑎\n",
        "+\n",
        "𝑏\n",
        "a+b times the mean of\n",
        "𝑋\n",
        "X.\n",
        "\n",
        "2. Show that $ \\text{cov}(X,X) = s^2$.\n",
        "The covariance of a variable with itself measures how much the variable varies around its own mean. This is exactly what variance captures. Since variance is defined as the average squared deviation from the mean, cov(X,X)\\text{cov}(X, X)cov(X,X) is equal to the sample variance s^2.\n",
        "\n",
        "\n",
        "3. Show that $\\text{cov}(X,a+bY) = b \\times \\text{cov}(X,Y)$\n",
        "\n",
        "Adding a constant to a variable does not change how it varies with another variable, because the relative differences stay the same. Multiplying by bbb scales those differences by a factor of b. As a result, the covariance between X and a+bY is just b times the covariance between X and Y.\n",
        "\n",
        "\n",
        "4. Show that $\\text{cov}(a+bX,a+bY) = b^2 \\text{cov}(X,Y) $. Notice, this also means that $\\text{cov}(bX, bX) = b^2 s^2$.\n",
        "\n",
        " When both variables are multiplied by bbb, each deviation from the mean is scaled by bbb, so their product is scaled by b^2. This means the covariance of a + bX and a + bY is b^2 times just the covariance of X and Y. In particular, this shows that cov(bX, bX) = b^2s^2\n",
        "\n",
        "5. Suppose $b>0$ and let the median of $X$ be $\\text{med}(X)$. Is it true that the median of $a+bX$ is equal to $a + b \\times \\text{med}(X)$? Is the IQR of $a + bX$ equal to $a + b \\times \\text{IQR}(X)$?\n",
        "\n",
        "When the multiplier is positive, multiplying every value of X keeps the order of the data the same, and adding a constant just shifts all values up or down. Because of this, the median changes in the same way as the data itself. The median of the transformed variable is equal to the constant plus the multiplier times the original median. The interquartile range measures spread, not location, so adding a constant does not change it. However, multiplying by the multiplier does scale the spread. Therefore, the interquartile range of the transformed variable is the multiplier times the original interquartile range, not the constant plus the multiplier times the interquartile range.\n",
        "\n",
        "6. Show by example that the means of $X^2$ and $\\sqrt{X}$ are generally not $(m(X))^2$ and $\\sqrt{m(X)}$. So, the results we derived above really depend on the linearity of the transformation $Y = a + bX$, and transformations like $Y = X^2$ or $Y = \\sqrt{X}$ will not behave in a similar way.\n",
        "\n",
        "The mean does not generally commute with nonlinear transformations. For example, if you square each value of a variable and then take the average, that result will usually be different from squaring the original average. The same idea applies to taking square roots: averaging the square roots of the values is not the same as taking the square root of the average. This happens because squaring and square roots change values in uneven ways, especially when the data are spread out. These examples show that the nice properties we proved earlier rely on the transformation being linear, and they do not hold for nonlinear transformations like squaring or taking square roots."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a5f54b39",
      "metadata": {
        "id": "a5f54b39"
      },
      "source": [
        "**Q7.** This question provides some practice doing exploratory data analysis and visualization.\n",
        "\n",
        "We'll use the `ames_prices.csv` dataset. The \"relevant\" variables for this question are:\n",
        "  - `price` - Sale price value of the house\n",
        "  - `Bldg.Type` - Building type of the house (single family home, end-of-unit townhome, duplex, interior townhome, two-family conversion)\n",
        "\n",
        "1. Load the `college_completion.csv` data with Pandas.\n",
        "2. Make a kernel density plot of price and compute a describe table. Now, make a kernel density plot of price conditional on building type, and use `.groupby()` to make a describe type for each type of building. Which building types are the most expensive, on average? Which have the highest variance in transaction prices?\n",
        "\n",
        "The kernel density plot of house prices shows a strongly right-skewed distribution, with most homes clustered around $150k–$200k and a long upper tail. When conditioning on building type, end-unit townhomes (TwnhsE) and single-family homes (1Fam) have the highest average prices, while two-family conversions (2fmCon) are the least expensive. Single-family homes also show the largest variance in prices, which is visible in both the wider KDE spread and the higher standard deviation.\n",
        "\n",
        "3. Make an ECDF plot of price, and compute the sample minimum, .25 quantile, median, .75 quantile, and sample maximum (i.e. a 5-number summary).\n",
        "\n",
        "The ECDF and five-number summary show a minimum price of $12,789, a first quartile of $129,500, a median of $160,000, a third quartile of $213,500, and a maximum of $755,000. This confirms that most homes are priced well below the extreme upper tail.\n",
        "\n",
        "4. Make a boxplot of price. Are there outliers? Make a boxplot of price conditional on building type. What patterns do you see?\n",
        "\n",
        "The overall boxplot of price clearly shows many high-price outliers. When broken down by building type, outliers are most prominent among single-family homes, which also have the widest interquartile range. Townhomes and duplexes tend to have tighter distributions and fewer extreme values.\n",
        "\n",
        "5. Make a dummy variable indicating that an observation is an outlier.\n",
        "\n",
        "Using the 1.5×IQR rule, 137 observations were flagged as outliers, while 2,793 were not. This shows that only a small fraction of homes drive the extreme upper tail of the price distribution.\n",
        "\n",
        "6. Winsorize the price variable, and compute a new kernel density plot and describe table. How do the results change?\n",
        "\n",
        "After winsorizing prices, the maximum price is capped at $335,000, which substantially reduces the influence of extreme values. The mean and standard deviation decrease, while the median and quartiles remain unchanged. The new kernel density plot is less skewed, indicating that winsorization smooths the distribution without affecting the typical home price."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "metadata": {
        "id": "qQ9yHdFpm6DX"
      },
      "id": "qQ9yHdFpm6DX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "ames = pd.read_csv(\"ames_prices.csv\")\n",
        "ames.head()"
      ],
      "metadata": {
        "id": "VOyQ6r9jnO07"
      },
      "id": "VOyQ6r9jnO07",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ames[\"price\"].plot(kind=\"kde\")\n",
        "plt.xlabel(\"House Price\")\n",
        "plt.title(\"Kernel Density of House Prices\")\n",
        "plt.show()\n",
        "\n",
        "ames[\"price\"].describe()"
      ],
      "metadata": {
        "id": "m4vXQKw8nXOA"
      },
      "id": "m4vXQKw8nXOA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for b in ames[\"Bldg.Type\"].unique():\n",
        "    ames[ames[\"Bldg.Type\"] == b][\"price\"].plot(kind=\"kde\", label=b)\n",
        "\n",
        "plt.legend()\n",
        "plt.xlabel(\"House Price\")\n",
        "plt.title(\"Price Density by Building Type\")\n",
        "plt.show()\n",
        "\n",
        "ames.groupby(\"Bldg.Type\")[\"price\"].describe()\n"
      ],
      "metadata": {
        "id": "EwWTUbW_ncKH"
      },
      "id": "EwWTUbW_ncKH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ames_sorted = ames[\"price\"].sort_values()\n",
        "ecdf = ames_sorted.rank(method=\"average\") / len(ames_sorted)\n",
        "\n",
        "plt.plot(ames_sorted, ecdf)\n",
        "plt.xlabel(\"House Price\")\n",
        "plt.ylabel(\"ECDF\")\n",
        "plt.title(\"ECDF of House Prices\")\n",
        "plt.show()\n",
        "\n",
        "ames[\"price\"].quantile([0, 0.25, 0.5, 0.75, 1])"
      ],
      "metadata": {
        "id": "Wg_yU5urni1w"
      },
      "id": "Wg_yU5urni1w",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ames.boxplot(column=\"price\")\n",
        "plt.title(\"Boxplot of House Prices\")\n",
        "plt.show()\n",
        "\n",
        "ames.boxplot(column=\"price\", by=\"Bldg.Type\", rot=45)\n",
        "plt.title(\"Price by Building Type\")\n",
        "plt.suptitle(\"\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "h7oL8MeUnngd"
      },
      "id": "h7oL8MeUnngd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q1 = ames[\"price\"].quantile(0.25)\n",
        "Q3 = ames[\"price\"].quantile(0.75)\n",
        "IQR = Q3 - Q1\n",
        "\n",
        "ames[\"outlier\"] = (\n",
        "    (ames[\"price\"] < Q1 - 1.5 * IQR) |\n",
        "    (ames[\"price\"] > Q3 + 1.5 * IQR)\n",
        ")\n",
        "\n",
        "ames[\"outlier\"].value_counts()"
      ],
      "metadata": {
        "id": "FTRRPl-Nnuca"
      },
      "id": "FTRRPl-Nnuca",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lower = ames[\"price\"].quantile(0.05)\n",
        "upper = ames[\"price\"].quantile(0.95)\n",
        "\n",
        "ames[\"price_wins\"] = ames[\"price\"].clip(lower, upper)\n",
        "\n",
        "ames[\"price_wins\"].plot(kind=\"kde\")\n",
        "plt.xlabel(\"Winsorized Price\")\n",
        "plt.title(\"KDE of Winsorized Prices\")\n",
        "plt.show()\n",
        "\n",
        "ames[\"price_wins\"].describe()\n"
      ],
      "metadata": {
        "id": "BeWnShFEnzf2"
      },
      "id": "BeWnShFEnzf2",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}